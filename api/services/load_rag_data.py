import json
import boto3
import os
import certifi
from pymongo.mongo_client import MongoClient
from dotenv import load_dotenv

# Initialize the Bedrock client (ensure your AWS credentials are properly set up)
bedrock_client = boto3.client('bedrock-runtime', region_name='us-east-1')

load_dotenv()

# MongoDB configuration
MONGO_URI = os.getenv("MONGO_URI")
DB_NAME = os.getenv("VECTOR_DB_NAME")
COLLECTION_NAME = os.getenv("VECTOR_COLLECTION_NAME")

# MongoDB client setup
ca = certifi.where()
client = MongoClient(MONGO_URI, tlsCAFile=ca)
db = client[DB_NAME]
collection = db[COLLECTION_NAME]

# Function to generate embedding using Amazon Titan model
def generate_embedding(text):
    try:
        native_request = {"inputText": text}
        request = json.dumps(native_request)

        response = bedrock_client.invoke_model(
            modelId='amazon.titan-embed-text-v1',  # Titan embed text model ID
            body=request,  # Input text for embedding generation
            contentType='application/json',  # Content type for the request
            accept='application/json'  # Expected response format
        )
        
        # Parse the response to get the embedding vector
        embedding = json.loads(response['body'].read().decode('utf-8'))['embedding']
        return embedding
    except Exception as e:
        return {str(e)}

def generate_answer_with_claude(prompt):
    """
    Sends the prompt to Claude Sonnet on AWS Bedrock and returns the response.
    
    Args:
        prompt (str): The prompt text for Claude to process.
    
    Returns:
        str: The response generated by Claude.
    """
    # Prepare request payload
    try:
        request_payload = {
            "prompt": f"\n\nHuman: {prompt}\n\nAssistant:",
            "max_tokens_to_sample": 1500,
            "temperature": 0.7
        }

        # Invoke Claude Sonnet model through Bedrock
        response = bedrock_client.invoke_model(
            modelId="anthropic.claude-instant-v1",  # Specify Claude's model ID
            body=json.dumps(request_payload),
            contentType="application/json",
            accept="application/json"
        )

        # Parse response body
        response_body = json.loads(response['body'].read().decode('utf-8'))
        answer = response_body.get("completion", "").strip()

        print(answer)

        return answer
    except Exception as e:
        return {str(e)}

# Define a function to run vector search queries
def get_query_results(query):
  """Gets results from a vector search query."""
  try:
    query_embedding = generate_embedding(query)
    pipeline = [
        {
                "$vectorSearch": {
                "index": "vector_index",
                "queryVector": query_embedding,
                "path": "embedding",
                "exact": True,
                "limit": 5
                }
        }, {
                "$project": {
                "_id": 0,
                "text": 1
            }
        }
    ]

    results = collection.aggregate(pipeline)

    array_of_results = []
    for doc in results:
        array_of_results.append(doc)
    return array_of_results
  except Exception as e:
        return {str(e)}

def construct_prompt(user_query):
    """
    Constructs a prompt for Claude based on sensor data and user query.
    
    Args:
        sensor_data (dict): Dictionary containing the sensor data and embedding vector.
        user_query (str): The user's question or query regarding the sensor data.
    
    Returns:
        str: The formatted prompt for Claude.
    """
    try:
        context_docs = get_query_results(user_query)
        print(context_docs)
        context_string = " ".join([doc["text"] for doc in context_docs])

        prompt = f"""Use the following pieces of context to answer the question at the end.
            {context_string}
            Question: {user_query}
        """
        # prompt = (
        #     f"Sensor ID: {sensor_data['sensor_id']}\n"
        #     f"Timestamp: {sensor_data['timestamp'].strftime('%Y-%m-%d %H:%M:%S UTC')}\n"
        #     f"Temperature: {sensor_data['temperature']}Â°C\n"
        #     f"Humidity: {sensor_data['humidity']}%\n"
        #     f"AQI: {sensor_data['AQI']}\n"
        #     f"CO2 Level: {sensor_data['CO2_level']} ppm\n"
        #     f"\nUser Query: {user_query}\n"
        #     f"\nContext: Based on the provided sensor readings, answer the user's query. "
        #     "Provide relevant insights or identify any abnormal readings. "
        #     "Use the embedding for contextual similarity if responding to related queries.\n"
        # )
        
        return prompt
    
    except Exception as e:
        return {str(e)}


def rag_query_pipeline(query_text):
    try:
        # Step 2: Construct prompt with retrieved documents
        prompt = construct_prompt(user_query=query_text)
        
        # Step 3: Generate answer using Claude
        answer = generate_answer_with_claude(prompt)
        
        return answer
    except Exception as e:
        return {str(e)}
